{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "start training......\n",
      "loss at epoch 0:\n",
      "cost: 2.320044736016572, accuracy: 0.33916853393085916\n",
      "loss at epoch 20:\n",
      "cost: 1.58747133730926, accuracy: 0.5495558578745213\n",
      "loss at epoch 40:\n",
      "cost: 1.5624749846708759, accuracy: 0.5622199103713199\n",
      "loss at epoch 60:\n",
      "cost: 1.5546895464770154, accuracy: 0.5673815620998734\n",
      "loss at epoch 80:\n",
      "cost: 1.5501884961555594, accuracy: 0.5714228553137022\n",
      "loss at epoch 100:\n",
      "cost: 1.5473740235180917, accuracy: 0.5714628681177991\n",
      "loss at epoch 120:\n",
      "cost: 1.545582364905964, accuracy: 0.5725632202304752\n",
      "loss at epoch 140:\n",
      "cost: 1.5451373309087826, accuracy: 0.5731233994878381\n",
      "loss at epoch 160:\n",
      "cost: 1.5447851636193017, accuracy: 0.5735435339308601\n",
      "loss at epoch 180:\n",
      "cost: 1.5444321542329587, accuracy: 0.573423495518567\n",
      "loss at epoch 200:\n",
      "cost: 1.5441992256919195, accuracy: 0.5733234635083252\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='latin1')\n",
    "    return d\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "data2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "data3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "data4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "data5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat and Dog image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image_path = [\"./dataset/cat_dog/cat\", \"./dataset/cat_dog/dog\"]\n",
    "#image_path = [\"./data/A\", \"./data/B\"]\n",
    "\n",
    "def preprocess_to_data_file(data_dir_list, ratio=0.8):\n",
    "\n",
    "    total_list = [] \n",
    "   \n",
    "    '''\n",
    "        create all images into (image path, label) format and write to total_data.txt \n",
    "    ''' \n",
    "    with open('total_data.txt', 'w') as f:\n",
    "        for index, data_dir in enumerate(data_dir_list):\n",
    "            for filename in listdir(data_dir):\n",
    "                #print(\"{} {}\".format(data_dir_list[index]+'/'+filename, index))\n",
    "                f.write('{} {}\\n'.format(data_dir_list[index]+'/'+filename.replace(' ',''), index))\n",
    "                total_list.append(data_dir_list[index]+'/'+filename.replace(' ','')+' '+str(index))\n",
    "\n",
    "    #print(total_list)\n",
    "    '''\n",
    "        shuffle total_list\n",
    "    ''' \n",
    "    shuffle(total_list)\n",
    "\n",
    "    '''\n",
    "        split total_list to train_list and test_list. \n",
    "        write train_list/test_list into train_data.txt/test_data.txt\n",
    "    ''' \n",
    "    train_list = total_list[:int(ratio*len(total_list))]\n",
    "    test_list = total_list[int(ratio*len(total_list)):]\n",
    "\n",
    "    with open('train_data.txt', 'w') as f:\n",
    "        for i in train_list:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "    with open('test_data.txt', 'w') as f:\n",
    "        for i in test_list:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "preprocess_to_data_file(image_path, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image path......\n",
      "number of train image is 20000\n",
      "number of test image is 5000\n",
      "start training......\n",
      "step=0, loss=12.495779037475586, accuracy=0.48046875\n",
      "test accuracy = 0.5090000033378601\n",
      "step=100, loss=1.1703170537948608, accuracy=0.52734375\n",
      "test accuracy = 0.5494999885559082\n",
      "step=200, loss=0.6291122436523438, accuracy=0.68359375\n",
      "test accuracy = 0.5824999809265137\n",
      "step=300, loss=0.6483730673789978, accuracy=0.67578125\n",
      "test accuracy = 0.578000009059906\n",
      "step=400, loss=0.7599363923072815, accuracy=0.6015625\n",
      "test accuracy = 0.6309999823570251\n",
      "step=500, loss=1.0419275760650635, accuracy=0.51953125\n",
      "test accuracy = 0.546500027179718\n",
      "step=600, loss=0.810225248336792, accuracy=0.5859375\n",
      "test accuracy = 0.531499981880188\n",
      "step=700, loss=0.5764691233634949, accuracy=0.67578125\n",
      "test accuracy = 0.6539999842643738\n",
      "step=800, loss=0.6346839666366577, accuracy=0.6640625\n",
      "test accuracy = 0.6690000295639038\n",
      "step=900, loss=0.5823110938072205, accuracy=0.67578125\n",
      "test accuracy = 0.6194999814033508\n",
      "step=1000, loss=0.6599491834640503, accuracy=0.6640625\n",
      "test accuracy = 0.6305000185966492\n",
      "step=1100, loss=0.5639418363571167, accuracy=0.70703125\n",
      "test accuracy = 0.5040000081062317\n",
      "step=1200, loss=1.0483946800231934, accuracy=0.54296875\n",
      "test accuracy = 0.6504999995231628\n",
      "step=1300, loss=0.6189267039299011, accuracy=0.68359375\n",
      "test accuracy = 0.6635000109672546\n",
      "step=1400, loss=0.5878932476043701, accuracy=0.703125\n",
      "test accuracy = 0.5385000109672546\n",
      "step=1500, loss=0.5932421088218689, accuracy=0.7109375\n",
      "test accuracy = 0.6884999871253967\n",
      "step=1600, loss=0.6259868741035461, accuracy=0.6484375\n",
      "test accuracy = 0.6919999718666077\n",
      "step=1700, loss=0.7757741808891296, accuracy=0.59765625\n",
      "test accuracy = 0.6959999799728394\n",
      "step=1800, loss=0.5367226600646973, accuracy=0.72265625\n",
      "test accuracy = 0.5615000128746033\n",
      "step=1900, loss=0.5068581104278564, accuracy=0.7109375\n",
      "test accuracy = 0.6520000100135803\n",
      "step=2000, loss=0.7774912118911743, accuracy=0.58984375\n",
      "test accuracy = 0.6194999814033508\n",
      "step=2100, loss=0.6088603734970093, accuracy=0.6796875\n",
      "test accuracy = 0.6915000081062317\n",
      "step=2200, loss=0.5111194252967834, accuracy=0.7265625\n",
      "test accuracy = 0.6880000233650208\n",
      "step=2300, loss=0.5452097654342651, accuracy=0.70703125\n",
      "test accuracy = 0.6970000267028809\n",
      "step=2400, loss=0.659247100353241, accuracy=0.65625\n",
      "test accuracy = 0.7269999980926514\n",
      "step=2500, loss=0.5713838934898376, accuracy=0.7265625\n",
      "test accuracy = 0.7279999852180481\n",
      "step=2600, loss=0.5025050640106201, accuracy=0.7265625\n",
      "test accuracy = 0.6324999928474426\n",
      "step=2700, loss=0.5312033295631409, accuracy=0.78125\n",
      "test accuracy = 0.6949999928474426\n",
      "step=2800, loss=0.7637815475463867, accuracy=0.62109375\n",
      "test accuracy = 0.6869999766349792\n",
      "step=2900, loss=0.5496158599853516, accuracy=0.7265625\n",
      "test accuracy = 0.7170000076293945\n",
      "step=3000, loss=0.5135052800178528, accuracy=0.74609375\n",
      "test accuracy = 0.7195000052452087\n",
      "step=3100, loss=0.5790759921073914, accuracy=0.71875\n",
      "test accuracy = 0.5764999985694885\n",
      "step=3200, loss=0.6339119076728821, accuracy=0.6796875\n",
      "test accuracy = 0.6710000038146973\n",
      "step=3300, loss=0.4993523061275482, accuracy=0.77734375\n",
      "test accuracy = 0.6660000085830688\n",
      "step=3400, loss=0.520248293876648, accuracy=0.7421875\n",
      "test accuracy = 0.6520000100135803\n",
      "step=3500, loss=0.5294527411460876, accuracy=0.7421875\n",
      "test accuracy = 0.7365000247955322\n",
      "step=3600, loss=0.5390185117721558, accuracy=0.73046875\n",
      "test accuracy = 0.7070000171661377\n",
      "step=3700, loss=0.7480044960975647, accuracy=0.62109375\n",
      "test accuracy = 0.7260000109672546\n",
      "step=3800, loss=0.8214407563209534, accuracy=0.62890625\n",
      "test accuracy = 0.6945000290870667\n",
      "step=3900, loss=0.5392242670059204, accuracy=0.72265625\n",
      "test accuracy = 0.6919999718666077\n",
      "step=4000, loss=0.5459315180778503, accuracy=0.7109375\n",
      "test accuracy = 0.6880000233650208\n",
      "step=4100, loss=0.5930835604667664, accuracy=0.703125\n",
      "test accuracy = 0.7429999709129333\n",
      "step=4200, loss=0.6813185811042786, accuracy=0.65234375\n",
      "test accuracy = 0.7480000257492065\n",
      "step=4300, loss=0.7876025438308716, accuracy=0.59765625\n",
      "test accuracy = 0.7225000262260437\n",
      "step=4400, loss=0.5367516875267029, accuracy=0.7421875\n",
      "test accuracy = 0.7494999766349792\n",
      "step=4500, loss=0.4487288296222687, accuracy=0.77734375\n",
      "test accuracy = 0.7419999837875366\n",
      "step=4600, loss=0.5899736881256104, accuracy=0.69140625\n",
      "test accuracy = 0.7225000262260437\n",
      "step=4700, loss=0.46011680364608765, accuracy=0.7890625\n",
      "test accuracy = 0.7609999775886536\n",
      "step=4800, loss=0.8776038885116577, accuracy=0.57421875\n",
      "test accuracy = 0.5680000185966492\n",
      "step=4900, loss=0.5271299481391907, accuracy=0.7578125\n",
      "test accuracy = 0.7544999718666077\n",
      "step=5000, loss=0.4967649281024933, accuracy=0.76953125\n",
      "test accuracy = 0.734000027179718\n",
      "step=5100, loss=0.527339518070221, accuracy=0.734375\n",
      "test accuracy = 0.621999979019165\n",
      "step=5200, loss=0.4550948143005371, accuracy=0.77734375\n",
      "test accuracy = 0.7509999871253967\n",
      "step=5300, loss=0.45319461822509766, accuracy=0.8046875\n",
      "test accuracy = 0.7519999742507935\n",
      "step=5400, loss=0.5166782736778259, accuracy=0.73046875\n",
      "test accuracy = 0.7559999823570251\n",
      "step=5500, loss=0.6413210034370422, accuracy=0.70703125\n",
      "test accuracy = 0.7639999985694885\n",
      "step=5600, loss=0.45841139554977417, accuracy=0.76953125\n",
      "test accuracy = 0.7565000057220459\n",
      "step=5700, loss=0.5707916617393494, accuracy=0.7109375\n",
      "test accuracy = 0.7419999837875366\n",
      "step=5800, loss=0.5823858380317688, accuracy=0.69921875\n",
      "test accuracy = 0.7475000023841858\n",
      "step=5900, loss=0.41933056712150574, accuracy=0.81640625\n",
      "test accuracy = 0.7574999928474426\n",
      "step=6000, loss=0.46267861127853394, accuracy=0.78125\n",
      "test accuracy = 0.7404999732971191\n",
      "step=6100, loss=0.4631307125091553, accuracy=0.796875\n",
      "test accuracy = 0.7404999732971191\n",
      "step=6200, loss=0.4930100440979004, accuracy=0.76953125\n",
      "test accuracy = 0.7509999871253967\n",
      "step=6300, loss=0.4582453966140747, accuracy=0.78125\n",
      "test accuracy = 0.699999988079071\n",
      "step=6400, loss=0.4949052929878235, accuracy=0.78515625\n",
      "test accuracy = 0.7005000114440918\n",
      "step=6500, loss=0.64743971824646, accuracy=0.671875\n",
      "test accuracy = 0.762499988079071\n",
      "step=6600, loss=0.4592142105102539, accuracy=0.8046875\n",
      "test accuracy = 0.7329999804496765\n",
      "step=6700, loss=0.5020135641098022, accuracy=0.7578125\n",
      "test accuracy = 0.7559999823570251\n",
      "step=6800, loss=0.6410528421401978, accuracy=0.67578125\n",
      "test accuracy = 0.7289999723434448\n",
      "step=6900, loss=0.5058730840682983, accuracy=0.78125\n",
      "test accuracy = 0.7540000081062317\n",
      "step=7000, loss=0.4872579276561737, accuracy=0.7578125\n",
      "test accuracy = 0.7360000014305115\n",
      "step=7100, loss=0.6329708099365234, accuracy=0.6875\n",
      "test accuracy = 0.7699999809265137\n",
      "step=7200, loss=0.523859441280365, accuracy=0.73828125\n",
      "test accuracy = 0.7689999938011169\n",
      "step=7300, loss=0.4232761263847351, accuracy=0.8125\n",
      "test accuracy = 0.6549999713897705\n",
      "step=7400, loss=0.4963800013065338, accuracy=0.7578125\n",
      "test accuracy = 0.7505000233650208\n",
      "step=7500, loss=0.510504424571991, accuracy=0.77734375\n",
      "test accuracy = 0.6765000224113464\n",
      "step=7600, loss=0.48668673634529114, accuracy=0.7578125\n",
      "test accuracy = 0.7664999961853027\n",
      "step=7700, loss=0.4967734217643738, accuracy=0.7578125\n",
      "test accuracy = 0.718999981880188\n",
      "step=7800, loss=0.45268183946609497, accuracy=0.77734375\n",
      "test accuracy = 0.7615000009536743\n",
      "step=7900, loss=0.4840521812438965, accuracy=0.77734375\n",
      "test accuracy = 0.7605000138282776\n",
      "step=8000, loss=0.5068092346191406, accuracy=0.75\n",
      "test accuracy = 0.7630000114440918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pb file to store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:46: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 accuracy= 0.05\n",
      "100 accuracy= 0.88\n",
      "200 accuracy= 0.92\n",
      "300 accuracy= 0.96\n",
      "400 accuracy= 0.99\n",
      "500 accuracy= 0.97\n",
      "600 accuracy= 0.94\n",
      "700 accuracy= 0.97\n",
      "800 accuracy= 0.94\n",
      "900 accuracy= 0.98\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:71: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "#定义输入数据mnist图片大小28*28*1=784,None表示batch_size\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,28*28],name=\"input\")\n",
    "#定义标签数据,mnist共10类\n",
    "y_ = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"y_\")\n",
    "#将数据调整为二维数据，w*H*c---> 28*28*1,-1表示N张\n",
    "image = tf.reshape(x,shape=[-1,28,28,1])\n",
    "\n",
    "#第一层，卷积核={5*5*1*32}，池化核={2*2*1,1*2*2*1}\n",
    "w1 = tf.Variable(initial_value=tf.random_normal(shape=[5,5,1,32],stddev=0.1,dtype=tf.float32,name=\"w1\"))\n",
    "b1= tf.Variable(initial_value=tf.zeros(shape=[32]))\n",
    "conv1 = tf.nn.conv2d(input=image,filter=w1,strides=[1,1,1,1],padding=\"SAME\",name=\"conv1\")\n",
    "relu1 = tf.nn.relu(tf.nn.bias_add(conv1,b1),name=\"relu1\")\n",
    "pool1 = tf.nn.max_pool(value=relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "#shape={None，14,14,32}\n",
    "#第二层，卷积核={5*5*32*64}，池化核={2*2*1,1*2*2*1}\n",
    "w2 = tf.Variable(initial_value=tf.random_normal(shape=[5,5,32,64],stddev=0.1,dtype=tf.float32,name=\"w2\"))\n",
    "b2 = tf.Variable(initial_value=tf.zeros(shape=[64]))\n",
    "conv2 = tf.nn.conv2d(input=pool1,filter=w2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "relu2 = tf.nn.relu(tf.nn.bias_add(conv2,b2),name=\"relu2\")\n",
    "pool2 = tf.nn.max_pool(value=relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\",name=\"pool2\")\n",
    "#shape={None，7,7,64}\n",
    "#FC1\n",
    "w3 = tf.Variable(initial_value=tf.random_normal(shape=[7*7*64,1024],stddev=0.1,dtype=tf.float32,name=\"w3\"))\n",
    "b3 = tf.Variable(initial_value=tf.zeros(shape=[1024]))\n",
    "#关键，进行reshape\n",
    "input3 = tf.reshape(pool2,shape=[-1,7*7*64],name=\"input3\")\n",
    "fc1 = tf.nn.relu(tf.nn.bias_add(value=tf.matmul(input3,w3),bias=b3),name=\"fc1\")\n",
    "#shape={None，1024}\n",
    "#FC2\n",
    "w4 = tf.Variable(initial_value=tf.random_normal(shape=[1024,10],stddev=0.1,dtype=tf.float32,name=\"w4\"))\n",
    "b4 = tf.Variable(initial_value=tf.zeros(shape=[10]))\n",
    "fc2 = tf.nn.bias_add(value=tf.matmul(fc1,w4),bias=b4)\n",
    "#shape={None，10}\n",
    "#定义交叉熵损失\n",
    "# 使用softmax将NN计算输出值表示为概率\n",
    "y = tf.nn.softmax(fc2,name=\"out\")\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2,labels=y_)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#定义solver\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss=loss)\n",
    "\n",
    "#定义正确值,判断二者下标index是否相等\n",
    "correct_predict = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "#定义如何计算准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict,dtype=tf.float32),name=\"accuracy\")\n",
    "\n",
    "\n",
    "#训练NN\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(0,1000):\n",
    "        xs, ys = mnist.train.next_batch(100)\n",
    "        session.run(fetches=train,feed_dict={x:xs,y_:ys})\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = session.run(fetches=accuracy,feed_dict={x:xs,y_:ys})\n",
    "            print(i,\"accuracy=\",train_accuracy)\n",
    "    #训练完成后，将网络中的权值转化为常量，形成常量graph\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess=session,\n",
    "                                                            input_graph_def=session.graph_def,\n",
    "                                                            output_node_names=['out'])\n",
    "    #将带权值的graph序列化，写成pb文件存储起来\n",
    "    with tf.gfile.FastGFile(\"lenet.pb\", mode='wb') as f:\n",
    "        f.write(constant_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "pre_label= [6]\n",
      "true label: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "pb_path = \"lenet.pb\"\n",
    "\n",
    "#导入pb文件到graph中\n",
    "with tf.gfile.FastGFile(pb_path,'rb') as f:\n",
    "    # 复制定义好的计算图到新的图中，先创建一个空的图.\n",
    "    graph_def = tf.GraphDef()\n",
    "    # 加载proto-buf中的模型\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    # 最后复制pre-def图的到默认图中.\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    #获取输入tensor\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    input_x = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    #获取预测tensor\n",
    "    output = tf.get_default_graph().get_tensor_by_name(\"out:0\")\n",
    "    #取第100张图片测试\n",
    "    one_image = np.reshape(mnist.test.images[100], [-1, 784])\n",
    "    #将测试图片传入nn中，做inference\n",
    "    out = session.run(output,feed_dict={input_x:one_image})\n",
    "    pre_label = np.argmax(out,1)\n",
    "    print(\"pre_label=\",pre_label)\n",
    "    print('true label:', np.argmax(mnist.test.labels[100],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
