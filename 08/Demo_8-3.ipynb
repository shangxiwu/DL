{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更精簡寫法的DNN神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3e74909f7f74>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shang\\.conda\\envs\\DL\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\shang\\.conda\\envs\\DL\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shang\\.conda\\envs\\DL\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shang\\.conda\\envs\\DL\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shang\\.conda\\envs\\DL\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-3e74909f7f74>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 1, cost= 84.1822954125837\n",
      "Epoch 2, cost= 17.60963545105676\n",
      "Epoch 3, cost= 10.602632184462115\n",
      "Epoch 4, cost= 7.436443161856052\n",
      "Epoch 5, cost= 5.59556255535647\n",
      "Epoch 6, cost= 4.3557563322782515\n",
      "Epoch 7, cost= 3.480566941499709\n",
      "Epoch 8, cost= 2.8078142194314433\n",
      "Epoch 9, cost= 2.2913314292004134\n",
      "Epoch 10, cost= 1.877336949143898\n",
      "Epoch 11, cost= 1.5401021962849937\n",
      "Epoch 12, cost= 1.2784207516447215\n",
      "Epoch 13, cost= 1.062739696812562\n",
      "Epoch 14, cost= 0.9026576387746771\n",
      "Epoch 15, cost= 0.7532838886277743\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9194999933242798\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15    #全部讀15次\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "  \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1= tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "out_1=tf.nn.relu(layer_1)\n",
    "\n",
    "layer_2= tf.add(tf.matmul(x,weights['h2']),biases['b2'])           \n",
    "out_2=tf.nn.relu(layer_2)\n",
    "\n",
    "out_layer =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "tf.argmax(y, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 534.7132455166901\n",
      "Epoch 2, cost= 357.5661748157846\n",
      "Epoch 3, cost= 267.4779227239434\n",
      "Epoch 4, cost= 204.60083121559833\n",
      "Epoch 5, cost= 157.70640034068714\n",
      "Epoch 6, cost= 121.34716444535708\n",
      "Epoch 7, cost= 92.52128053144968\n",
      "Epoch 8, cost= 69.52201370239256\n",
      "Epoch 9, cost= 51.248351294777564\n",
      "Epoch 10, cost= 36.93492619601165\n",
      "Epoch 11, cost= 25.946591370322498\n",
      "Epoch 12, cost= 17.72882160706952\n",
      "Epoch 13, cost= 11.776669542139228\n",
      "Epoch 14, cost= 7.621183482950387\n",
      "Epoch 15, cost= 4.845591530366374\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9387999773025513\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15     #100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "  \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "beta = 0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\\\n",
    "                      +beta*tf.nn.l2_loss(weights['h1'])\n",
    "                      +beta*tf.nn.l2_loss(weights['h2'])\n",
    "                      +beta*tf.nn.l2_loss(weights['out'])     #加入L2正則向\n",
    "                     )\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 120.53170233639797\n",
      "Epoch 2, cost= 34.453706653768386\n",
      "Epoch 3, cost= 21.860089945359665\n",
      "Epoch 4, cost= 16.263246495506973\n",
      "Epoch 5, cost= 12.9839444286173\n",
      "Epoch 6, cost= 10.799296731948854\n",
      "Epoch 7, cost= 8.971669114719736\n",
      "Epoch 8, cost= 7.799213991598653\n",
      "Epoch 9, cost= 6.731116567524995\n",
      "Epoch 10, cost= 5.94700772003694\n",
      "Epoch 11, cost= 5.185937258547002\n",
      "Epoch 12, cost= 4.624648597890681\n",
      "Epoch 13, cost= 4.285671879378232\n",
      "Epoch 14, cost= 3.7321682696992675\n",
      "Epoch 15, cost= 3.400674555084921\n",
      "Epoch 16, cost= 3.1659055929834157\n",
      "Epoch 17, cost= 2.939180097038097\n",
      "Epoch 18, cost= 2.7165935258431846\n",
      "Epoch 19, cost= 2.4616052935340145\n",
      "Epoch 20, cost= 2.3659174264561016\n",
      "Epoch 21, cost= 2.15761536143043\n",
      "Epoch 22, cost= 2.0606288899074894\n",
      "Epoch 23, cost= 1.9609397620504572\n",
      "Epoch 24, cost= 1.8450091307813477\n",
      "Epoch 25, cost= 1.729359647035599\n",
      "Epoch 26, cost= 1.707328972166235\n",
      "Epoch 27, cost= 1.5941226528991355\n",
      "Epoch 28, cost= 1.5278662949258632\n",
      "Epoch 29, cost= 1.502612213654952\n",
      "Epoch 30, cost= 1.4550855596498997\n",
      "Epoch 31, cost= 1.3845364804701388\n",
      "Epoch 32, cost= 1.3883331045237448\n",
      "Epoch 33, cost= 1.3266940498352053\n",
      "Epoch 34, cost= 1.2850837808305557\n",
      "Epoch 35, cost= 1.2338676533915758\n",
      "Epoch 36, cost= 1.2242368275468987\n",
      "Epoch 37, cost= 1.1755673878843123\n",
      "Epoch 38, cost= 1.1503204139796195\n",
      "Epoch 39, cost= 1.1615621831200336\n",
      "Epoch 40, cost= 1.1126962791789647\n",
      "Epoch 41, cost= 1.0933319804885175\n",
      "Epoch 42, cost= 1.0895209550857545\n",
      "Epoch 43, cost= 1.086899074641141\n",
      "Epoch 44, cost= 1.052768241058696\n",
      "Epoch 45, cost= 1.0234890730814494\n",
      "Epoch 46, cost= 1.035796948346226\n",
      "Epoch 47, cost= 1.0064904655109752\n",
      "Epoch 48, cost= 1.0022883035919887\n",
      "Epoch 49, cost= 1.013828443939035\n",
      "Epoch 50, cost= 0.9915615455670785\n",
      "Epoch 51, cost= 0.991034675944936\n",
      "Epoch 52, cost= 0.961425127983093\n",
      "Epoch 53, cost= 0.9638835731419637\n",
      "Epoch 54, cost= 0.9604275366392998\n",
      "Epoch 55, cost= 0.954595696601001\n",
      "Epoch 56, cost= 0.943053612492301\n",
      "Epoch 57, cost= 0.9380800175666817\n",
      "Epoch 58, cost= 0.9437766126069156\n",
      "Epoch 59, cost= 0.924585730812766\n",
      "Epoch 60, cost= 0.9261103707010099\n",
      "Epoch 61, cost= 0.9292283286831594\n",
      "Epoch 62, cost= 0.918451180783185\n",
      "Epoch 63, cost= 0.9164298846504906\n",
      "Epoch 64, cost= 0.9101820944656024\n",
      "Epoch 65, cost= 0.9127923200347213\n",
      "Epoch 66, cost= 0.9058088489012284\n",
      "Epoch 67, cost= 0.9089298750053758\n",
      "Epoch 68, cost= 0.895002465681597\n",
      "Epoch 69, cost= 0.9108035686883048\n",
      "Epoch 70, cost= 0.8998882220008146\n",
      "Epoch 71, cost= 0.8884612956914042\n",
      "Epoch 72, cost= 0.890567072088068\n",
      "Epoch 73, cost= 0.8832740738175128\n",
      "Epoch 74, cost= 0.8952717453783207\n",
      "Epoch 75, cost= 0.880356322093444\n",
      "Epoch 76, cost= 0.8983247305046426\n",
      "Epoch 77, cost= 0.8788843127814204\n",
      "Epoch 78, cost= 0.87656782171943\n",
      "Epoch 79, cost= 0.8938526421243493\n",
      "Epoch 80, cost= 0.8884042365984487\n",
      "Epoch 81, cost= 0.8822561434182256\n",
      "Epoch 82, cost= 0.8800196659564975\n",
      "Epoch 83, cost= 0.8666532006046983\n",
      "Epoch 84, cost= 0.8808702337741844\n",
      "Epoch 85, cost= 0.8706546754186808\n",
      "Epoch 86, cost= 0.8703032011335542\n",
      "Epoch 87, cost= 0.8685359168052682\n",
      "Epoch 88, cost= 0.8709472229263995\n",
      "Epoch 89, cost= 0.8793004830317068\n",
      "Epoch 90, cost= 0.8728735335306693\n",
      "Epoch 91, cost= 0.8703454879197212\n",
      "Epoch 92, cost= 0.8670627271045338\n",
      "Epoch 93, cost= 0.8605576101216411\n",
      "Epoch 94, cost= 0.8708931994438166\n",
      "Epoch 95, cost= 0.8723736705563288\n",
      "Epoch 96, cost= 0.8640834666382196\n",
      "Epoch 97, cost= 0.8632147436792201\n",
      "Epoch 98, cost= 0.8616384620016272\n",
      "Epoch 99, cost= 0.8679812060702934\n",
      "Epoch 100, cost= 0.8581449899890202\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532999992370605\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs =  15     #100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# add dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "pred_drop = tf.nn.dropout(pred, keep_prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_drop, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
