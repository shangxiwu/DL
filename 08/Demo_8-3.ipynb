{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更精簡寫法的DNN神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 68.84904951268977\n",
      "Epoch 2, cost= 15.52693415901877\n",
      "Epoch 3, cost= 9.472174798358571\n",
      "Epoch 4, cost= 6.683460513028234\n",
      "Epoch 5, cost= 5.020318177721714\n",
      "Epoch 6, cost= 3.9153551186214783\n",
      "Epoch 7, cost= 3.1145799239115246\n",
      "Epoch 8, cost= 2.5318835242769917\n",
      "Epoch 9, cost= 2.0619819707220253\n",
      "Epoch 10, cost= 1.6925577483664838\n",
      "Epoch 11, cost= 1.3992290512269197\n",
      "Epoch 12, cost= 1.1626638468777737\n",
      "Epoch 13, cost= 0.9582060029912237\n",
      "Epoch 14, cost= 0.809961411288955\n",
      "Epoch 15, cost= 0.6820422463274076\n",
      "Optimization Finished!\n",
      "Accuracy: 0.917900025844574\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15    #全部讀15次\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "  \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 549.0087350741293\n",
      "Epoch 2, cost= 370.35553416859\n",
      "Epoch 3, cost= 283.0331598177824\n",
      "Epoch 4, cost= 222.08057853005144\n",
      "Epoch 5, cost= 176.27485767711286\n",
      "Epoch 6, cost= 140.1663731522992\n",
      "Epoch 7, cost= 110.86507941506123\n",
      "Epoch 8, cost= 86.85204889470893\n",
      "Epoch 9, cost= 67.12495991793546\n",
      "Epoch 10, cost= 51.06898301558059\n",
      "Epoch 11, cost= 38.17787578929556\n",
      "Epoch 12, cost= 28.00605390722103\n",
      "Epoch 13, cost= 20.136367690346468\n",
      "Epoch 14, cost= 14.170388594540674\n",
      "Epoch 15, cost= 9.76602852474559\n",
      "Epoch 16, cost= 6.591648005572232\n",
      "Epoch 17, cost= 4.384870691299442\n",
      "Epoch 18, cost= 2.9103253225846712\n",
      "Epoch 19, cost= 1.969826926968315\n",
      "Epoch 20, cost= 1.389101837331598\n",
      "Epoch 21, cost= 1.0422681379318235\n",
      "Epoch 22, cost= 0.837076479413293\n",
      "Epoch 23, cost= 0.7156019766764209\n",
      "Epoch 24, cost= 0.6461747073043479\n",
      "Epoch 25, cost= 0.6032039717110721\n",
      "Epoch 26, cost= 0.5796629978309981\n",
      "Epoch 27, cost= 0.5639473636583854\n",
      "Epoch 28, cost= 0.5524954281611878\n",
      "Epoch 29, cost= 0.544356532259421\n",
      "Epoch 30, cost= 0.5387792443145406\n",
      "Epoch 31, cost= 0.5349161886085165\n",
      "Epoch 32, cost= 0.5327612383799119\n",
      "Epoch 33, cost= 0.5307483914765442\n",
      "Epoch 34, cost= 0.527636553536762\n",
      "Epoch 35, cost= 0.5274036023833534\n",
      "Epoch 36, cost= 0.5265286545861851\n",
      "Epoch 37, cost= 0.5263267391378226\n",
      "Epoch 38, cost= 0.5222045686570079\n",
      "Epoch 39, cost= 0.5225065949288279\n",
      "Epoch 40, cost= 0.5210874019969594\n",
      "Epoch 41, cost= 0.5203760491176084\n",
      "Epoch 42, cost= 0.5213181807236235\n",
      "Epoch 43, cost= 0.5187466531991961\n",
      "Epoch 44, cost= 0.5199961644952944\n",
      "Epoch 45, cost= 0.5195117132230241\n",
      "Epoch 46, cost= 0.5179847835410722\n",
      "Epoch 47, cost= 0.517690107497302\n",
      "Epoch 48, cost= 0.5168095746907326\n",
      "Epoch 49, cost= 0.5176622305674979\n",
      "Epoch 50, cost= 0.5166450426253409\n",
      "Epoch 51, cost= 0.5154539278420537\n",
      "Epoch 52, cost= 0.5164805163578553\n",
      "Epoch 53, cost= 0.5148674108223488\n",
      "Epoch 54, cost= 0.5147493365677918\n",
      "Epoch 55, cost= 0.5143583914366638\n",
      "Epoch 56, cost= 0.5146339654922486\n",
      "Epoch 57, cost= 0.5142043638771231\n",
      "Epoch 58, cost= 0.5153122635863039\n",
      "Epoch 59, cost= 0.5136913049221031\n",
      "Epoch 60, cost= 0.5143256052515723\n",
      "Epoch 61, cost= 0.51547697706656\n",
      "Epoch 62, cost= 0.5135921743241225\n",
      "Epoch 63, cost= 0.5139076813242653\n",
      "Epoch 64, cost= 0.5142675997994162\n",
      "Epoch 65, cost= 0.5138588170571763\n",
      "Epoch 66, cost= 0.513896922360766\n",
      "Epoch 67, cost= 0.5132575103369624\n",
      "Epoch 68, cost= 0.5128154533559628\n",
      "Epoch 69, cost= 0.5134712417017326\n",
      "Epoch 70, cost= 0.512533483234319\n",
      "Epoch 71, cost= 0.5133356702869585\n",
      "Epoch 72, cost= 0.5131354217637669\n",
      "Epoch 73, cost= 0.5128674456206233\n",
      "Epoch 74, cost= 0.5125613246180798\n",
      "Epoch 75, cost= 0.5131732188571588\n",
      "Epoch 76, cost= 0.5129822656783192\n",
      "Epoch 77, cost= 0.512387777783654\n",
      "Epoch 78, cost= 0.5124688231945037\n",
      "Epoch 79, cost= 0.5124809141592547\n",
      "Epoch 80, cost= 0.5120831737735058\n",
      "Epoch 81, cost= 0.5121749466657639\n",
      "Epoch 82, cost= 0.5120918345993218\n",
      "Epoch 83, cost= 0.5117523207447744\n",
      "Epoch 84, cost= 0.5119586789608002\n",
      "Epoch 85, cost= 0.5108907056938518\n",
      "Epoch 86, cost= 0.5111377359520303\n",
      "Epoch 87, cost= 0.511622462381016\n",
      "Epoch 88, cost= 0.5115789685466072\n",
      "Epoch 89, cost= 0.5111739494583825\n",
      "Epoch 90, cost= 0.5124221110885795\n",
      "Epoch 91, cost= 0.5108348306742577\n",
      "Epoch 92, cost= 0.5100176330588082\n",
      "Epoch 93, cost= 0.512086644443599\n",
      "Epoch 94, cost= 0.5109554877606305\n",
      "Epoch 95, cost= 0.5109495857087056\n",
      "Epoch 96, cost= 0.5114057273756367\n",
      "Epoch 97, cost= 0.5109269312836906\n",
      "Epoch 98, cost= 0.5102543376250699\n",
      "Epoch 99, cost= 0.5115885906869712\n",
      "Epoch 100, cost= 0.511011257930235\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9477999806404114\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15     #100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "  \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "beta = 0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\\\n",
    "                      +beta*tf.nn.l2_loss(weights['h1'])\n",
    "                      +beta*tf.nn.l2_loss(weights['h2'])\n",
    "                      +beta*tf.nn.l2_loss( weights['out'])\n",
    "                     )\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 120.53170233639797\n",
      "Epoch 2, cost= 34.453706653768386\n",
      "Epoch 3, cost= 21.860089945359665\n",
      "Epoch 4, cost= 16.263246495506973\n",
      "Epoch 5, cost= 12.9839444286173\n",
      "Epoch 6, cost= 10.799296731948854\n",
      "Epoch 7, cost= 8.971669114719736\n",
      "Epoch 8, cost= 7.799213991598653\n",
      "Epoch 9, cost= 6.731116567524995\n",
      "Epoch 10, cost= 5.94700772003694\n",
      "Epoch 11, cost= 5.185937258547002\n",
      "Epoch 12, cost= 4.624648597890681\n",
      "Epoch 13, cost= 4.285671879378232\n",
      "Epoch 14, cost= 3.7321682696992675\n",
      "Epoch 15, cost= 3.400674555084921\n",
      "Epoch 16, cost= 3.1659055929834157\n",
      "Epoch 17, cost= 2.939180097038097\n",
      "Epoch 18, cost= 2.7165935258431846\n",
      "Epoch 19, cost= 2.4616052935340145\n",
      "Epoch 20, cost= 2.3659174264561016\n",
      "Epoch 21, cost= 2.15761536143043\n",
      "Epoch 22, cost= 2.0606288899074894\n",
      "Epoch 23, cost= 1.9609397620504572\n",
      "Epoch 24, cost= 1.8450091307813477\n",
      "Epoch 25, cost= 1.729359647035599\n",
      "Epoch 26, cost= 1.707328972166235\n",
      "Epoch 27, cost= 1.5941226528991355\n",
      "Epoch 28, cost= 1.5278662949258632\n",
      "Epoch 29, cost= 1.502612213654952\n",
      "Epoch 30, cost= 1.4550855596498997\n",
      "Epoch 31, cost= 1.3845364804701388\n",
      "Epoch 32, cost= 1.3883331045237448\n",
      "Epoch 33, cost= 1.3266940498352053\n",
      "Epoch 34, cost= 1.2850837808305557\n",
      "Epoch 35, cost= 1.2338676533915758\n",
      "Epoch 36, cost= 1.2242368275468987\n",
      "Epoch 37, cost= 1.1755673878843123\n",
      "Epoch 38, cost= 1.1503204139796195\n",
      "Epoch 39, cost= 1.1615621831200336\n",
      "Epoch 40, cost= 1.1126962791789647\n",
      "Epoch 41, cost= 1.0933319804885175\n",
      "Epoch 42, cost= 1.0895209550857545\n",
      "Epoch 43, cost= 1.086899074641141\n",
      "Epoch 44, cost= 1.052768241058696\n",
      "Epoch 45, cost= 1.0234890730814494\n",
      "Epoch 46, cost= 1.035796948346226\n",
      "Epoch 47, cost= 1.0064904655109752\n",
      "Epoch 48, cost= 1.0022883035919887\n",
      "Epoch 49, cost= 1.013828443939035\n",
      "Epoch 50, cost= 0.9915615455670785\n",
      "Epoch 51, cost= 0.991034675944936\n",
      "Epoch 52, cost= 0.961425127983093\n",
      "Epoch 53, cost= 0.9638835731419637\n",
      "Epoch 54, cost= 0.9604275366392998\n",
      "Epoch 55, cost= 0.954595696601001\n",
      "Epoch 56, cost= 0.943053612492301\n",
      "Epoch 57, cost= 0.9380800175666817\n",
      "Epoch 58, cost= 0.9437766126069156\n",
      "Epoch 59, cost= 0.924585730812766\n",
      "Epoch 60, cost= 0.9261103707010099\n",
      "Epoch 61, cost= 0.9292283286831594\n",
      "Epoch 62, cost= 0.918451180783185\n",
      "Epoch 63, cost= 0.9164298846504906\n",
      "Epoch 64, cost= 0.9101820944656024\n",
      "Epoch 65, cost= 0.9127923200347213\n",
      "Epoch 66, cost= 0.9058088489012284\n",
      "Epoch 67, cost= 0.9089298750053758\n",
      "Epoch 68, cost= 0.895002465681597\n",
      "Epoch 69, cost= 0.9108035686883048\n",
      "Epoch 70, cost= 0.8998882220008146\n",
      "Epoch 71, cost= 0.8884612956914042\n",
      "Epoch 72, cost= 0.890567072088068\n",
      "Epoch 73, cost= 0.8832740738175128\n",
      "Epoch 74, cost= 0.8952717453783207\n",
      "Epoch 75, cost= 0.880356322093444\n",
      "Epoch 76, cost= 0.8983247305046426\n",
      "Epoch 77, cost= 0.8788843127814204\n",
      "Epoch 78, cost= 0.87656782171943\n",
      "Epoch 79, cost= 0.8938526421243493\n",
      "Epoch 80, cost= 0.8884042365984487\n",
      "Epoch 81, cost= 0.8822561434182256\n",
      "Epoch 82, cost= 0.8800196659564975\n",
      "Epoch 83, cost= 0.8666532006046983\n",
      "Epoch 84, cost= 0.8808702337741844\n",
      "Epoch 85, cost= 0.8706546754186808\n",
      "Epoch 86, cost= 0.8703032011335542\n",
      "Epoch 87, cost= 0.8685359168052682\n",
      "Epoch 88, cost= 0.8709472229263995\n",
      "Epoch 89, cost= 0.8793004830317068\n",
      "Epoch 90, cost= 0.8728735335306693\n",
      "Epoch 91, cost= 0.8703454879197212\n",
      "Epoch 92, cost= 0.8670627271045338\n",
      "Epoch 93, cost= 0.8605576101216411\n",
      "Epoch 94, cost= 0.8708931994438166\n",
      "Epoch 95, cost= 0.8723736705563288\n",
      "Epoch 96, cost= 0.8640834666382196\n",
      "Epoch 97, cost= 0.8632147436792201\n",
      "Epoch 98, cost= 0.8616384620016272\n",
      "Epoch 99, cost= 0.8679812060702934\n",
      "Epoch 100, cost= 0.8581449899890202\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9532999992370605\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs =  15     #100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# add dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "pred_drop = tf.nn.dropout(pred, keep_prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_drop, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"Accuracy: {}\".format(accuracy_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
