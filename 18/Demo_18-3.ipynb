{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow unstack使用 \n",
    "\n",
    "### unstack at axis = 0\n",
    "![Alt text](./images/basic_tensorflow/unstack_axis0.png)\n",
    "### unstack at axis = 1\n",
    "![Alt text](./images/basic_tensorflow/unstack_axis1.png)\n",
    "### unstack at axis = 2\n",
    "![Alt text](./images/basic_tensorflow/unstack_axis2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before unstack......\n",
      "[[0.7 0.9]\n",
      " [0.1 0.4]\n",
      " [0.5 0.8]]\n",
      "after unstack......\n",
      "[0.7 0.9]\n",
      "[0.1 0.4]\n",
      "[0.5 0.8]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([[0.7,0.9],[0.1,0.4],[0.5,0.8]], name='x')\n",
    "axis0_x = tf.unstack(x, axis=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_, axis0_ = sess.run([x, axis0_x])\n",
    "    print('before unstack......')\n",
    "    print(x_)\n",
    "    print('after unstack......')\n",
    "    print(axis0_[0])\n",
    "    print(axis0_[1])\n",
    "    print(axis0_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow stack使用\n",
    "\n",
    "### origin data\n",
    "![Alt text](./images/basic_tensorflow/stack_origin.png)\n",
    "### stack at axis = 0\n",
    "![Alt text](./images/basic_tensorflow/stack_axis0.png)\n",
    "### stack at axis = 1\n",
    "![Alt text](./images/basic_tensorflow/stack_axis1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 2.]\n",
      "  [3. 4.]\n",
      "  [5. 6.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [0. 1.]\n",
      "  [1. 1.]]]\n",
      "(2, 3, 2)\n",
      "\n",
      "=================\n",
      "\n",
      "[[[1. 2.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[3. 4.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[5. 6.]\n",
      "  [1. 1.]]]\n",
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], name='x')\n",
    "y = tf.constant([[1.0, 1.0], [0.0, 1.0], [1.0, 1.0]], name='y')\n",
    "\n",
    "stacked_axis0_result = tf.stack([x,y], axis=0)\n",
    "stacked_axis1_result = tf.stack([x,y], axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    stacked_axis0_result_, stacked_axis1_result_ = sess.run([stacked_axis0_result, stacked_axis1_result])\n",
    "    print(stacked_axis0_result_)\n",
    "    print(stacked_axis0_result_.shape)\n",
    "    print('\\n'+ '=================' +'\\n')\n",
    "    print(stacked_axis1_result_)\n",
    "    print(stacked_axis1_result_.shape)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN實作MNIST分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 1, Minibatch Loss= 3.5396, Training Accuracy= 0.125\n",
      "Step 200, Minibatch Loss= 0.9783, Training Accuracy= 0.703\n",
      "Step 400, Minibatch Loss= 0.8239, Training Accuracy= 0.797\n",
      "Step 600, Minibatch Loss= 0.4811, Training Accuracy= 0.836\n",
      "Step 800, Minibatch Loss= 0.4628, Training Accuracy= 0.859\n",
      "Step 1000, Minibatch Loss= 0.4570, Training Accuracy= 0.859\n",
      "Step 1200, Minibatch Loss= 0.4950, Training Accuracy= 0.883\n",
      "Step 1400, Minibatch Loss= 0.3042, Training Accuracy= 0.914\n",
      "Step 1600, Minibatch Loss= 0.3756, Training Accuracy= 0.883\n",
      "Step 1800, Minibatch Loss= 0.4167, Training Accuracy= 0.859\n",
      "Step 2000, Minibatch Loss= 0.4963, Training Accuracy= 0.812\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8984375\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 2000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "\n",
    "    # Define a cell with tensorflow\n",
    "    cell=tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
    "    #cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    #cell = tf.contrib.rnn.GRUCell(num_hidden)\n",
    "    \n",
    "    ## switch to multiple layer RNN\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(num_hidden) for _ in range(3)])\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    # Get cell output\n",
    "    \n",
    "    #if inputs is (batches, steps, inputs) ==> time_major=False\n",
    "    #if inputs is (steps, batches, inputs) ==> time_major=True\n",
    "    #state_is_tuple = Treu mean output of dynamic_rnn is (c_state, h_state)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state, time_major=False)\n",
    "    # change outputs to list [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits , 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
